{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "chat-session.schema.json",
  "title": "ChatSession",
  "description": "Full conversation session stored at .mcp-client-data/chats/YYYY-MM-DD/chat-HHMMSS-{sessionId}.json",
  "type": "object",
  "required": ["sessionId", "startTime", "model", "servers", "messages", "metadata"],
  "properties": {
    "sessionId": {
      "type": "string",
      "description": "Unique session identifier"
    },
    "startTime": {
      "type": "string",
      "format": "date-time",
      "description": "ISO 8601 timestamp when session started"
    },
    "endTime": {
      "type": "string",
      "format": "date-time",
      "description": "ISO 8601 timestamp when session ended"
    },
    "model": {
      "type": "string",
      "description": "LLM model identifier used for this session"
    },
    "servers": {
      "type": "array",
      "items": { "type": "string" },
      "description": "MCP server names connected during session"
    },
    "tools": {
      "type": "array",
      "description": "Available tools during the session",
      "items": { "$ref": "tool.schema.json" }
    },
    "messages": {
      "type": "array",
      "description": "Ordered conversation messages",
      "items": { "$ref": "#/$defs/ChatMessage" }
    },
    "tokenUsagePerCallback": {
      "type": "array",
      "description": "Token usage recorded per LLM callback",
      "items": { "$ref": "#/$defs/TokenUsageRecord" }
    },
    "metadata": { "$ref": "#/$defs/SessionMetadata" }
  },
  "$defs": {
    "OrchestratorWorkflow": {
      "description": "Orchestrator call pattern (not a stored type — documents how messages relate). The agent writes Python code and calls mcp-tools-orchestrator__execute_composed_code. The orchestrator executes that code in a subprocess. The code calls MCP tools as plain functions (e.g. ros_mcp_server__move_home()), which route back to the client via IPC — no second LLM is involved. This produces two kinds of tool messages in the messages array: (1) The parent: a tool message with toolName='mcp-tools-orchestrator__execute_composed_code', toolInput.code containing the Python script, and toolOutput containing the combined stdout/stderr as JSON {output, returncode, status}. (2) The children: N tool messages with isIPCCall=true, one per MCP tool call made by the code. These are logged to chat history but NOT sent back to the LLM context — the LLM only sees the parent's toolOutput. To group children under their parent, filter IPC messages whose toolInputTime falls within the parent's [toolInputTime, toolOutputTime] window. Cost: only the 2 LLM callbacks (write code + read result) incur token cost. The N IPC tool executions have zero LLM cost since they are direct MCP calls with no model in the loop.",
      "type": "null"
    },
    "ChatMessage": {
      "type": "object",
      "required": ["timestamp", "role", "content"],
      "properties": {
        "timestamp": {
          "type": "string",
          "format": "date-time",
          "description": "ISO 8601 timestamp of this message"
        },
        "role": {
          "type": "string",
          "enum": ["user", "assistant", "tool", "client"],
          "description": "Message author role"
        },
        "content": {
          "type": "string",
          "description": "Text content of the message"
        },
        "thinking": {
          "type": "string",
          "description": "Accumulated thinking/reasoning content from model (Anthropic extended thinking, OpenAI reasoning_content, Gemini thought parts, Ollama thinking)"
        },
        "content_blocks": {
          "type": "array",
          "description": "Structured content blocks preserving tool_use blocks from assistant",
          "items": {
            "type": "object",
            "properties": {
              "type": { "type": "string" }
            },
            "additionalProperties": true
          }
        },
        "tool_use_id": {
          "type": "string",
          "description": "Tool use ID for pairing tool results with tool calls"
        },
        "attachments": {
          "type": "array",
          "description": "File attachments included with this message",
          "items": {
            "type": "object",
            "required": ["fileName", "ext", "mediaType"],
            "properties": {
              "fileName": { "type": "string" },
              "ext": { "type": "string", "description": "File extension (e.g. .png, .pdf)" },
              "mediaType": { "type": "string", "description": "MIME type" }
            }
          }
        },
        "toolName": {
          "type": "string",
          "description": "Name of the tool called (on tool messages)"
        },
        "toolInput": {
          "type": "object",
          "description": "Arguments passed to the tool. For orchestrator calls (toolName='mcp-tools-orchestrator__execute_composed_code'), contains {code: string} with the Python script the agent wrote.",
          "additionalProperties": true
        },
        "toolOutput": {
          "type": "string",
          "description": "Raw output returned by the tool. For orchestrator calls, this is a JSON string with {output: string, returncode: number, status: 'success'|'failed'} containing the script's stdout/stderr. This is the only result the LLM sees — individual IPC tool outputs are not sent to the LLM context."
        },
        "toolInputTime": {
          "type": "string",
          "format": "date-time",
          "description": "When tool input was sent"
        },
        "toolOutputTime": {
          "type": "string",
          "format": "date-time",
          "description": "When tool output was received"
        },
        "orchestratorMode": {
          "type": "boolean",
          "description": "Whether this tool call occurred while orchestrator mode was active. True on both the parent execute_composed_code message and its child IPC messages. See OrchestratorWorkflow."
        },
        "isIPCCall": {
          "type": "boolean",
          "description": "Whether this tool call was made by orchestrator code via IPC (a child call). These messages are logged to chat history but not sent to the LLM context. To find the parent, find the execute_composed_code message whose [toolInputTime, toolOutputTime] window contains this message's toolInputTime. See OrchestratorWorkflow."
        }
      }
    },
    "TokenUsageRecord": {
      "type": "object",
      "required": ["timestamp", "inputTokens", "outputTokens", "totalTokens"],
      "properties": {
        "timestamp": { "type": "string", "format": "date-time" },
        "inputTokens": { "type": "integer" },
        "outputTokens": { "type": "integer" },
        "totalTokens": { "type": "integer" },
        "regularInputTokens": { "type": "integer", "description": "Non-cached input tokens" },
        "cacheCreationTokens": { "type": "integer", "description": "Tokens used to create cache" },
        "cacheReadTokens": { "type": "integer", "description": "Tokens read from cache" },
        "estimatedCost": { "type": "number", "description": "Estimated cost in USD" },
        "ollamaMetrics": {
          "type": "object",
          "description": "Ollama-specific performance metrics",
          "properties": {
            "totalDuration": { "type": "number", "description": "Total inference duration (ns)" },
            "loadDuration": { "type": "number", "description": "Model load duration (ns)" },
            "evalDuration": { "type": "number", "description": "Response generation duration (ns)" },
            "promptEvalDuration": { "type": "number", "description": "Prompt evaluation duration (ns)" },
            "evalCount": { "type": "integer", "description": "Number of tokens generated" },
            "promptEvalCount": { "type": "integer", "description": "Number of prompt tokens evaluated" },
            "evalRate": { "type": "number", "description": "Tokens/second for generation" },
            "promptEvalRate": { "type": "number", "description": "Tokens/second for prompt eval" }
          }
        }
      }
    },
    "SessionMetadata": {
      "type": "object",
      "required": ["messageCount", "toolUseCount", "ipcCallCount"],
      "properties": {
        "cumulativeTokens": { "type": "integer", "description": "Total tokens used across session" },
        "peakContextTokens": { "type": "integer", "description": "Peak context window usage" },
        "messageCount": { "type": "integer" },
        "toolUseCount": { "type": "integer", "description": "Direct tool calls made by the LLM (excludes IPC calls)" },
        "ipcCallCount": { "type": "integer", "description": "Tool calls made by orchestrator code via IPC. These have zero LLM cost. See OrchestratorWorkflow." },
        "totalCost": { "type": "number", "description": "Total estimated LLM cost in USD. Only covers direct LLM callbacks (tokenUsagePerCallback). IPC tool calls have no LLM cost since they are executed without a model." }
      }
    }
  }
}
